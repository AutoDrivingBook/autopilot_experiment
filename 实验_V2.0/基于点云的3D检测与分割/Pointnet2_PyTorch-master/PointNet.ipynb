{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于PointNet++的点云分类和语义分割实验\n",
    "随着卷积神经网络 CNN 的快速发展,CNN 在二维图像领域取得了十分优秀的成果。但推广到三维领域,无论是成倍增加的网络体积和计算量,还是数据存储量,都是影响卷积神经网络在三维领域发展的障碍。与二维图像上的检测分割不同的是,3D 检测的任务主要是确定可以表示某一种类目标姿态的 3D 边界框,它即包含目标的空间位置信息,也包含目标的朝向、旋转状态等信息;3D分割的任务主要是分割点云,区分不同种类的物体,将整个点云划分为各种在语义上有意义的部分或是各个有意义的个体。\n",
    "\n",
    "点云数据是一个由无序的数据点构成的集合,因此点云数据上的深度学习一直是一个较为困难的任务。在使用深度学习模型处理点云数据之前,往往需要对点云数据进行处理。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointNet\n",
    "PointNet 是 Qi 等人在 2017 年 CVPR 会议的一篇文章中提出的方法,其主要完成 3D 检测和分割\n",
    "PointNet是Qi等人在2017年CVPR会议的一篇文章中提出的方法，其主要完成3D检测和分割任务。PointNet提出直接在点云数据上应用深度学习模型，并且用实验证明其高准确率性能。PointNet主要针对点云数据的无序性和空间变换的不变性进行论证，并提出了很好的解决方法。文章详情见PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation。\n",
    "无序性指点云数据是由可任意排列的数据组成的集合，而要使用深度学习模型的一个前提是需要保证不论点云顺序如何，都应该提取得到相同的特征。在PointNet中作者提出用Maxpooling对称函数来提取特征，解决无序性问题。Maxpooling即在每一维的特征都选取N个点中对应的最大的特征值。\n",
    "变换不变性是指点云数据所表示的目标经过一定的空间变换（旋转、平移等）后应该保持不变，在坐标系中即为点云数据坐标发生变化后，不论其用何种的坐标系表示，网络都能正确地识别目标。在PointNet中作者提出在对点云数据提取特征前先用STN（Spatial Transform Network）对齐以保证其不变性。STN如图6-27中两转换网络示意图所示，T-Net为了让特征对齐，会通过学习点云的位置信息来找到最适合的旋转角度，得到一个转换矩阵，并通过将其与输入点云数据相乘来保证数据的不变性。\n",
    "PointNet的网络结构如下图所示，其主要流程为：\n",
    "\n",
    "1. 输入数据为（为点云个数，对应点云的空间坐标）的点云数据集合，并将其表示为的张量。\n",
    "2. 首先将输入数据通过一个STN，其将T-Net学习到的转换矩阵与输入数据相乘来进行对齐以保证其对一定的空间变换保持不变性。\n",
    "3. 通过多层感知机来提取点云数据特征，将其特征维数升到64维后，再通过一个STN对齐特征。\n",
    "4. 再次通过多层感知机MLP提取点云数据特征，将其特征维数升到1024维后，使用Maxpooling得到其全局特征。\n",
    "5. 若目的为分类，则直接将全局特征通过多层感知机得到各个种类的预测分数；若目的为分割，则将全局特征与之前提取到的64维特征（局部特征）进行串联，在通过多层感知机对每个点进行分类。\n",
    "\n",
    " ![PointNet网络结构图](images/PointNet.png)\n",
    " \n",
    "PointNet++是Charles发表在机器学习会议NIPS（2017）的一篇文章，其思想基于改进PointNet局部特征提取变现不好的缺点，提出了一种分层神经网络结构（Hierarchical Neural Nettwork），这种分层结构有一系列的set abstraction组成，如图6-28所示，其包括采样层（Sampling layer）、组合层（Grouping layer）和特征提取层（PointNet layer）。PointNet++在不同尺度下提取特征作为局部特征，并通过多层网络结构得到深层特征。文章详情见PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space。\n",
    "\n",
    "![PointNet++网络结构图](images/PointNet++.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验环境\n",
    "本实验在erikwijmans的PointNet2_PyTorch上改进而来。\n",
    "### 1.0 Python = 3.5\n",
    "本实验使用Python 3.5及其以上版本的Python\n",
    "### 1.1 Pytorch = 0.4.1\n",
    "本实验使用Pytorch 0.4.1以上版本，读者需要根据自己计算机的设备选择合适的CUDA版本，或CPU版本\n",
    "### 1.2 Visodm\n",
    "本实验需要使用Visdom可视化工具，可实时监控训练进程，更加直观的查看训练过程中准确度和损失函数的变化，从而在整体上判断训练过程是否过拟合等。\n",
    "Visdom是一种可视化工具，支持Pytorch和numpy，可以实现远程数据的实时的可视化，清楚的显示各个数据随时间的变化，如图6-31所示，为本次实验训练PointNet++分割模型过程中的可视化界面。Visdom界面如下图所示：\n",
    "![visdom](images/visdom.png)\n",
    "\n",
    "通过以下命令来安装visdom:\n",
    "```shell\n",
    "pip3 install visdom\n",
    "```\n",
    "\n",
    "安装好visdom之后，在命令行终端输入```python -m visdom.server```来开启端口：\n",
    "\n",
    "![vidom端口](images/visdom端口.png)\n",
    "\n",
    "在本实验中，由于实验环境部署在docker中，因此，需要将```8097```端口映射到宿主机的端口上，才能在浏览器中实时显示训练过程的loss变换。本实验中对visdom类的定义在```pointnet2/utils/pytoch_utils/visdomvis.py```中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic-Kitti数据集\n",
    "本实验使用著名的Kitti数据集来作为激光点云的分割数据集。Semantic-Kitti是基于KITTI Vision Benchmark提出的一个大规模激光点云语义分割和全景分割数据集，Semantic-Kitti的作者为KITTI的00-10序列进行了密集的语义标注，使得可以使用多个顺序扫描进行语义场景解释，例如语义分段和语义场景完成。而KITTI的其余的序列，即序列11-21，则被用作测试集，显示了各种具有挑战性的交通状况和环境类型。\n",
    "\n",
    "![训练集](images/overview-train.svg)\n",
    "\n",
    "![测试集](images/overview-test.svg)\n",
    "\n",
    "Semantic-Kitti中包含28类不同的语义标签，包括了静止目标和动态目标，其标签分布如下所示：\n",
    "![标签分布](images/label_distribution.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 网络结构代码\n",
    "\n",
    "本部分将使用pytorch来搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络结构如下图所示：\n",
    "\n",
    "![网络结构](images/PointNet++.png)\n",
    "\n",
    "### 3.1  Set Abstraction层\n",
    "PointNet++中，基础网络层是由若干个Set Abstraction层构成。这些曾简称SA层，由一个Sampling Layer、Grouping Layer和一个Pointnet组件构成。为了方便搭建和修改网络，我们需要先构建SA层。\n",
    "\n",
    "下面介绍一下Set Abstraction的具体组成：\n",
    "* 采样层：即在处理点云数据之前，先进行采样处理。在该文章中，Charles使用迭代最远点采样（Farthest Point Sampling，FPS）方法。该方法根据选定的一个点云数据子集，对子集中每个点抽取其距离最远的点，得到一个新的子集，这两个子集的并集就是样本空间。与随机取样相比，FPS能更好的覆盖整个点云数据。采样层的输入为$N\\times(d + C)$，其中d为点云的坐标，C为特征维度，N为输入点数量\n",
    "* 组合层：其根据采样点定义了一个局部域，方便下一步在该区域中提取特征。其输入为的点云集合以及个质心的坐标，输出为的点云集合，其中是局部区域除了质心点之外的点数。在卷积神经网络中，一张图像像素的局部区域指像素周围曼哈顿距离下的领域点，而点云中一个点的局部区域指该点几何距离下的邻域点组成。与临近算法不同的是，该文章用了Ball Query方法，其选取固定半径区域内的点（设有上限）。\n",
    "* 特征提取层：即使用PointNet对组合层中的局部区域提取特征。在PointNet++代码中，这一部分进行了化简，使用了一个max_pool2d来作为特征提取层。\n",
    "\n",
    "另外，不同于图像中各个像素点密度均匀分布，点云数据在空间上分布密度不均匀、不规则，在距离较远的地方激光点云十分稀疏。无差别的提取特征会使得网络很容易在低密度的地方丢失局部信息，而导致PointNet训练效果不理想，造成较大的误差。因此，在点云密度较小的地方应该加大选取的局部区域来更好的提取特征。Charles提出密度自适应PointNet层，来组合不同密度点云数据的特征。文章中提出了两种组合的方式：多尺度组合（Multi-scale Grouping, MSG）和多分辨率组合（Multi-resolution Grouping, MRG）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SA层的基础结构\n",
    "class _PointnetSAModuleBase(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.npoint = None\n",
    "        self.groupers = None\n",
    "        self.mlps = None\n",
    "        \n",
    "    def forward(self, xyz: torch.Tensor,features: torch.Tensor = None) -> (torch.Tensor, torch.Tensor):\n",
    "        r\"\"\"\n",
    "        Parameters\n",
    "        xyz : torch.Tensor\n",
    "                    输入维度(B, N, 3)，数据的三维特征表示\n",
    "        features : torch.Tensor \n",
    "                    输入维度(B, N, C)，特征表示,B 为batchsize，N为点云数目，3为三维坐标\n",
    "        xyz与feature共同构成了SA层的输入\n",
    "        Returns\n",
    "        new_xyz : torch.Tensor\n",
    "                    输出维度(B, npoint, 3)，分组点云的三维表示\n",
    "        new_features : torch.Tensor(B, npoint, \\sum_k(mlps[k][-1]))，每组点云的特征表示\n",
    "        new_xyz与new_features共同构成了SA层的输出\n",
    "        \"\"\"\n",
    "        new_features_list = []\n",
    "        # 对点云进行下采样\n",
    "        xyz_flipped = xyz.transpose(1, 2).contiguous()\n",
    "        new_xyz = pointnet2_utils.gather_operation(\n",
    "            xyz_flipped,\n",
    "            pointnet2_utils.furthest_point_sample(xyz, self.npoint)\n",
    "        ).transpose(1, 2).contiguous() if self.npoint is not None else None\n",
    "        # 计算经过最远点采样和分组处理之后的新点云数据\n",
    "        for i in range(len(self.groupers)):\n",
    "            # groupper层的\n",
    "            new_features = self.groupers[i](\n",
    "                xyz, new_xyz, features)  \n",
    "            # (B, C, npoint, nsample)\n",
    "            new_features = self.mlps[i](new_features)  \n",
    "            # (B, mlp[-1], npoint, nsample)\n",
    "            new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)]) \n",
    "            # (B, mlp[-1], npoint, 1)\n",
    "            new_features = new_features.squeeze(-1) \n",
    "\t\t # (B, mlp[-1], npoint)\n",
    "            new_features_list.append(new_features)\n",
    "        #提取特征过程中先分组，对每一分组，用多层感知机提取特征，最后使用全局最大池化提取全局特征。\n",
    "        return new_xyz, torch.cat(new_features_list, dim=1)\n",
    "        #返回分组后的数据以及存有每组特征的列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "述代码中，使用了pointnet2_utils中定义的gather_operation和furthest_point_sample方法，来实现对点云的下采样。\n",
    "\n",
    "在定义好_PointNetSAModuleBase类之后，接下来就要正式定义SA层了。PointNet++的SA层分为两种，一种是基于多尺度Grouping的SA层，一种是基于单尺度Grouping的SA层。二者的定义如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# 多尺度Grouping的SA层\n",
    "class PointnetSAModuleMSG(_PointnetSAModuleBase):\n",
    "    r\"\"\"Pointnet set abstrction layer with multiscale grouping\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    npoint : int\n",
    "        Number of features\n",
    "    radii : list of float32\n",
    "        list of radii to group with\n",
    "    nsamples : list of int32\n",
    "        Number of samples in each ball query\n",
    "    mlps : list of list of int32\n",
    "        Spec of the pointnet before the global max_pool for each scale\n",
    "    bn : bool\n",
    "        Use batchnorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,*,npoint: int,radii: List[float],nsamples: List[int], mlps: List[List[int]], bn: bool = True,  use_xyz: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(radii) == len(nsamples) == len(mlps)\n",
    "\n",
    "        self.npoint = npoint\n",
    "        self.groupers = nn.ModuleList()\n",
    "        self.mlps = nn.ModuleList()\n",
    "        for i in range(len(radii)):\n",
    "            radius = radii[i]\n",
    "            nsample = nsamples[i]\n",
    "            self.groupers.append(\n",
    "                pointnet2_utils.QueryAndGroup(radius, nsample, use_xyz=use_xyz)\n",
    "                if npoint is not None else pointnet2_utils.GroupAll(use_xyz)\n",
    "            )\n",
    "            mlp_spec = mlps[i]\n",
    "            if use_xyz:\n",
    "                mlp_spec[0] += 3\n",
    "\n",
    "            self.mlps.append(pt_utils.SharedMLP(mlp_spec, bn=bn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单尺度Grouping的SA层，直接继承了PointnetSAModuleMSG层\n",
    "class PointnetSAModule(PointnetSAModuleMSG):\n",
    "    r\"\"\"Pointnet set abstrction layer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    npoint : int\n",
    "        Number of features\n",
    "    radius : float\n",
    "        Radius of ball\n",
    "    nsample : int\n",
    "        Number of samples in the ball query\n",
    "    mlp : list\n",
    "        Spec of the pointnet before the global max_pool\n",
    "    bn : bool\n",
    "        Use batchnorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            mlp: List[int],\n",
    "            npoint: int = None,\n",
    "            radius: float = None,\n",
    "            nsample: int = None,\n",
    "            bn: bool = True,\n",
    "            use_xyz: bool = True\n",
    "    ):\n",
    "        super().__init__(\n",
    "            mlps=[mlp],\n",
    "            npoint=npoint,\n",
    "            radii=[radius],\n",
    "            nsamples=[nsample],\n",
    "            bn=bn,\n",
    "            use_xyz=use_xyz\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Segmentation层\n",
    "分割任务中，SA层之后，还需要接一个Segmentation网络，以实现分割。Segementation通过skip line connection操作不断与底层的低阶信息融合，最终实现对原数据中每一个点云的语义分割结果。\n",
    "\n",
    "由于在SA层中，对原始点云进行了若干次下采样。因此，需要在Segmentation网络中进行上采样，以还原原始数据。PointNet++中采用基于距离的插值和across level skip links的分层传播策略来将分割结果传播到原始点云数据中。在特征传播级别中，作者将点特征从$N_l \\times (d + C)$个点传播到个$N_{l - 1}$个点，其中$N_{l-1}$和$N_l$分别为第$l$级SA层的输入和输出的点云的数量。\n",
    "\n",
    "Segementation网络中，基本网路单元定义为FP层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointnetFPModule(nn.Module):\n",
    "    r\"\"\"Propigates the features of one set to another\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mlp : list\n",
    "        Pointnet module parameters\n",
    "    bn : bool\n",
    "        Use batchnorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, mlp: List[int], bn: bool = True):\n",
    "        super().__init__()\n",
    "        self.mlp = pt_utils.SharedMLP(mlp, bn=bn)\n",
    "\n",
    "    def forward(\n",
    "            self, unknown: torch.Tensor, known: torch.Tensor,\n",
    "            unknow_feats: torch.Tensor, known_feats: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        unknown : torch.Tensor\n",
    "            (B, n, 3) tensor of the xyz positions of the unknown features\n",
    "        known : torch.Tensor\n",
    "            (B, m, 3) tensor of the xyz positions of the known features\n",
    "        unknow_feats : torch.Tensor\n",
    "            (B, C1, n) tensor of the features to be propigated to\n",
    "        known_feats : torch.Tensor\n",
    "            (B, C2, m) tensor of features to be propigated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        new_features : torch.Tensor\n",
    "            (B, mlp[-1], n) tensor of the features of the unknown features\n",
    "        \"\"\"\n",
    "\n",
    "        if known is not None:\n",
    "            dist, idx = pointnet2_utils.three_nn(unknown, known)\n",
    "            dist_recip = 1.0 / (dist + 1e-8)\n",
    "            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n",
    "            weight = dist_recip / norm\n",
    "\n",
    "            interpolated_feats = pointnet2_utils.three_interpolate(\n",
    "                known_feats, idx, weight\n",
    "            )\n",
    "        else:\n",
    "            interpolated_feats = known_feats.expand(\n",
    "                *known_feats.size()[0:2], unknown.size(1)\n",
    "            )\n",
    "\n",
    "        if unknow_feats is not None:\n",
    "            new_features = torch.cat([interpolated_feats, unknow_feats],\n",
    "                                   dim=1)  #(B, C2 + C1, n)\n",
    "        else:\n",
    "            new_features = interpolated_feats\n",
    "\n",
    "        new_features = new_features.unsqueeze(-1)\n",
    "        new_features = self.mlp(new_features)\n",
    "\n",
    "        return new_features.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 完整的PointNet++\n",
    "至此，我们已经完成PointNet++网络中的结构单元的定义，截下来可以定义一个PointNet++网络了。\n",
    "\n",
    "根据不同组合，PointNet++有四种不同网络结构：多尺度策略的分类和分割网络、单尺度的分类和分割网络。我们以多尺度策略的分割网络为例，来构建一个PointNet++网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointnet2_msg_sem网络结构\n",
    "\n",
    "class Pointnet2MSG(nn.Module):\n",
    "    r\"\"\"\n",
    "        PointNet2 with multi-scale grouping\n",
    "        Semantic segmentation network that uses feature propogation layers\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes: int\n",
    "            Number of semantics classes to predict over -- size of softmax classifier that run for each point\n",
    "        input_channels: int = 6\n",
    "            Number of input channels in the feature descriptor for each point.  If the point cloud is Nx9, this\n",
    "            value should be 6 as in an Nx9 point cloud, 3 of the channels are xyz, and 6 are feature descriptors\n",
    "        use_xyz: bool = True\n",
    "            Whether or not to use the xyz position of a point as a feature\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, input_channels=6, use_xyz=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.SA_modules = nn.ModuleList()\n",
    "        \n",
    "        # 1st SA Layer\n",
    "        c_in = input_channels\n",
    "        self.SA_modules.append(\n",
    "            PointnetSAModuleMSG(\n",
    "                npoint=1024,\n",
    "                radii=[0.05, 0.1],\n",
    "                nsamples=[16, 32],\n",
    "                mlps=[[c_in, 16, 16, 32], [c_in, 32, 32, 64]],\n",
    "                use_xyz=use_xyz\n",
    "            )\n",
    "        )\n",
    "        c_out_0 = 32 + 64\n",
    "        \n",
    "        # 2nd SA Layer\n",
    "        c_in = c_out_0\n",
    "        self.SA_modules.append(\n",
    "            PointnetSAModuleMSG(\n",
    "                npoint=256,\n",
    "                radii=[0.1, 0.2],\n",
    "                nsamples=[16, 32],\n",
    "                mlps=[[c_in, 64, 64, 128], [c_in, 64, 96, 128]],\n",
    "                use_xyz=use_xyz\n",
    "            )\n",
    "        )\n",
    "        c_out_1 = 128 + 128\n",
    "        \n",
    "        # 3rn SA Layer\n",
    "        c_in = c_out_1\n",
    "        self.SA_modules.append(\n",
    "            PointnetSAModuleMSG(\n",
    "                npoint=64,\n",
    "                radii=[0.2, 0.4],\n",
    "                nsamples=[16, 32],\n",
    "                mlps=[[c_in, 128, 196, 256], [c_in, 128, 196, 256]],\n",
    "                use_xyz=use_xyz\n",
    "            )\n",
    "        )\n",
    "        c_out_2 = 256 + 256\n",
    "        \n",
    "        # 4th SA Layer\n",
    "        c_in = c_out_2\n",
    "        self.SA_modules.append(\n",
    "            PointnetSAModuleMSG(\n",
    "                npoint=16,\n",
    "                radii=[0.4, 0.8],\n",
    "                nsamples=[16, 32],\n",
    "                mlps=[[c_in, 256, 256, 512], [c_in, 256, 384, 512]],\n",
    "                use_xyz=use_xyz\n",
    "            )\n",
    "        )\n",
    "        c_out_3 = 512 + 512\n",
    "\n",
    "        self.FP_modules = nn.ModuleList()\n",
    "        # 1st FP Layer\n",
    "        self.FP_modules.append( PointnetFPModule(mlp=[256 + input_channels, 128, 128]))\n",
    "        # 2nd FP Layer\n",
    "        self.FP_modules.append(PointnetFPModule(mlp=[512 + c_out_0, 256, 256]))\n",
    "        # 3rd FP Layer\n",
    "        self.FP_modules.append(PointnetFPModule(mlp=[512 + c_out_1, 512, 512]))\n",
    "        # 4th FP Layer\n",
    "        self.FP_modules.append( PointnetFPModule(mlp=[c_out_3 + c_out_2, 512, 512]))\n",
    "        \n",
    "        # 全连接层\n",
    "        self.FC_layer = nn.Sequential(\n",
    "            pt_utils.Conv1d(128, 128, bn=True), nn.Dropout(),\n",
    "            pt_utils.Conv1d(128, num_classes, activation=None)\n",
    "        )\n",
    "    \n",
    "    def _break_up_pc(self, pc):\n",
    "        xyz = pc[..., 0:3].contiguous()\n",
    "        features = (\n",
    "            pc[..., 3:].transpose(1, 2).contiguous()\n",
    "            if pc.size(-1) > 3 else None\n",
    "        )\n",
    "\n",
    "        return xyz, features\n",
    "\n",
    "    def forward(self, pointcloud: torch.cuda.FloatTensor):\n",
    "        r\"\"\"\n",
    "            Forward pass of the network\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            pointcloud: Variable(torch.cuda.FloatTensor)\n",
    "                (B, N, 3 + input_channels) tensor\n",
    "                Point cloud to run predicts on\n",
    "                Each point in the point-cloud MUST\n",
    "                be formated as (x, y, z, features...)\n",
    "        \"\"\"\n",
    "        xyz, features = self._break_up_pc(pointcloud)\n",
    "\n",
    "        l_xyz, l_features = [xyz], [features]\n",
    "        # pdb.set_trace()\n",
    "        for i in range(len(self.SA_modules)):\n",
    "            li_xyz, li_features = self.SA_modules[i](l_xyz[i], l_features[i])\n",
    "            l_xyz.append(li_xyz)\n",
    "            l_features.append(li_features)\n",
    "\n",
    "        for i in range(-1, -(len(self.FP_modules) + 1), -1):\n",
    "            l_features[i - 1] = self.FP_modules[i](\n",
    "                l_xyz[i - 1], l_xyz[i], l_features[i - 1], l_features[i]\n",
    "            )\n",
    "\n",
    "        return self.FC_layer(l_features[0]).transpose(1, 2).contiguous()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Semantic-Kitti数据集的分割任务实验\n",
    "本部分使用PointNet++，在Kitti数据集上进行分割实验。\n",
    "\n",
    "本实验中，选择序列00的前100帧激光点云来进行实验。本实验的数据位于```dataset```文件夹下，其结构如下所示：\n",
    "\n",
    "![数据文件夹结构](images/folder_structure.svg)\n",
    "\n",
    "```0000xx.bin```文件为点云文件，```0000xx.label```为点云中每个点对应的标签。我们使用jbehley等人所提供的工具来进行数据的读写(https://github.com/PRBonn/semantic-kitti-api )，这个API在```Pointnet2_PyTorch-master/pointnet2/data/kitti_api```中。\n",
    "\n",
    "基于上述的API，我们构建了一个```torch.data.Dataset```类来负责训练和测试时的数据读取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "(10059200, 3)\n",
      "9799219\n",
      "(100, 100592, 9)\n",
      "(100, 100592)\n",
      "19\n",
      "(tensor([[52.8979,  0.0230,  1.9980, 52.8979,  0.0230,  1.9980, 52.8979,  0.0230,\n",
      "          1.9980],\n",
      "        [53.7505,  0.1929,  2.0270, 53.7505,  0.1929,  2.0270, 53.7505,  0.1929,\n",
      "          2.0270],\n",
      "        [53.8031,  0.3618,  2.0289, 53.8031,  0.3618,  2.0289, 53.8031,  0.3618,\n",
      "          2.0289],\n",
      "        [72.6007,  1.2965,  2.6647, 72.6007,  1.2965,  2.6647, 72.6007,  1.2965,\n",
      "          2.6647],\n",
      "        [72.1183,  1.5134,  2.6477, 72.1183,  1.5134,  2.6477, 72.1183,  1.5134,\n",
      "          2.6477],\n",
      "        [74.4768,  1.7983,  2.7276, 74.4768,  1.7983,  2.7276, 74.4768,  1.7983,\n",
      "          2.7276],\n",
      "        [73.2674,  1.9992,  2.6876, 73.2674,  1.9992,  2.6876, 73.2674,  1.9992,\n",
      "          2.6876],\n",
      "        [72.1100,  2.1931,  2.6485, 72.1100,  2.1931,  2.6485, 72.1100,  2.1931,\n",
      "          2.6485],\n",
      "        [72.9786,  2.4500,  2.6775, 72.9786,  2.4500,  2.6775, 72.9786,  2.4500,\n",
      "          2.6775],\n",
      "        [72.9362,  2.6779,  2.6764, 72.9362,  2.6779,  2.6764, 72.9362,  2.6779,\n",
      "          2.6764],\n",
      "        [63.7448, 11.1593,  2.3956, 63.7448, 11.1593,  2.3956, 63.7448, 11.1593,\n",
      "          2.3956],\n",
      "        [63.7866, 11.2703,  2.3976, 63.7866, 11.2703,  2.3976, 63.7866, 11.2703,\n",
      "          2.3976],\n",
      "        [64.3973, 11.5871,  2.4195, 64.3973, 11.5871,  2.4195, 64.3973, 11.5871,\n",
      "          2.4195],\n",
      "        [64.3309, 11.7841,  2.4185, 64.3309, 11.7841,  2.4185, 64.3309, 11.7841,\n",
      "          2.4185],\n",
      "        [63.8395, 11.9010,  2.4035, 63.8395, 11.9010,  2.4035, 63.8395, 11.9010,\n",
      "          2.4035],\n",
      "        [63.3401, 12.0140,  2.3874, 63.3401, 12.0140,  2.3874, 63.3401, 12.0140,\n",
      "          2.3874]]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "100\n",
      "torch.Size([4, 16, 9])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import glob\n",
    "import yaml\n",
    "\n",
    "\n",
    "from pointnet2.data.kitti_api.auxiliary.laserscan import *\n",
    "\n",
    "class Kitti3DSemSeg(data.Dataset):\n",
    "    def __init__(self, num_points, train = True, download = True, data_precent = 1.0):\n",
    "        super().__init__()\n",
    "        self.data_precent = data_precent\n",
    "        self.train, self.num_points = train, num_points\n",
    "\n",
    "        pcd_path = '/kitti_semantic/dataset/sequences/00/velodyne/'\n",
    "        label_path = '/kitti_semantic/dataset/sequences/00/labels/'\n",
    "        pcd_list, label_list = [], []\n",
    "        points, labels = [], []\n",
    "        # 写入pcd_path的路径\n",
    "        pcd_list = glob.glob(os.path.join(pcd_path, '*.bin'))\n",
    "        # 写入label_path的路径\n",
    "        label_list = glob.glob(os.path.join(label_path, '*.label'))\n",
    "        # 将文件list重排序\n",
    "        pcd_list.sort()\n",
    "        label_list.sort()\n",
    "\n",
    "        # 验证pcd_list与label_list的长度是否相同\n",
    "        assert(len(pcd_list) == len(label_list))\n",
    "        num_of_file = len(pcd_list)\n",
    "\n",
    "        # 读取点云及其标签\n",
    "        config_path = '/kitti_semantic/Pointnet2_PyTorch-master/pointnet2/data/kitti_api/config/semantic-kitti.yaml'\n",
    "        CFG = yaml.safe_load(open(config_path, 'r'))\n",
    "        color_dict = CFG[\"color_map\"]\n",
    "        class_remap = CFG[\"learning_map\"]\n",
    "\n",
    "        nclasses = len(color_dict)\n",
    "        print(nclasses)\n",
    "        sem_ls = SemLaserScan(nclasses, color_dict, project = False)\n",
    "\n",
    "        k = 0\n",
    "        for i in range(0, num_of_file):\n",
    "            scan_file = pcd_list[i]\n",
    "            label_file = label_list[i]\n",
    "            \n",
    "            sem_ls.open_scan(scan_file)\n",
    "            sem_ls.open_label(label_file)\n",
    "            scans = sem_ls.points.tolist()\n",
    "            sems = sem_ls.sem_label.astype(np.float32).tolist()\n",
    "            # 每片点云中选择100592个点\n",
    "            if len(scans) >= 100592 and len(sems) >= 100592:\n",
    "                points.extend(scans[0 : 100592])\n",
    "                labels.extend(sems[0 : 100592])\n",
    "                k += 1\n",
    "        \n",
    "        points = np.array(points)\n",
    "        print(points.shape)\n",
    "        points = np.reshape(points, (k, 100592, 3))\n",
    "        self.points = np.tile(points, (1, 1, 3))\n",
    "        labels_remap = []\n",
    "        for x in labels:\n",
    "            labels_remap.append(class_remap[x])\n",
    "        labels = np.array(labels_remap)\n",
    "        print(len(labels.nonzero()[0]))\n",
    "        self.labels = np.reshape(labels, (num_of_file, 100592))\n",
    "\n",
    "        print(self.points.shape)\n",
    "        print(self.labels.shape)\n",
    "        print(max(labels))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pt_idxs = np.arange(0, self.num_points)\n",
    "        # np.random.shuffle(pt_idxs)\n",
    "\n",
    "        current_points = torch.from_numpy(self.points[idx, pt_idxs].copy()\n",
    "                                         ).type(torch.FloatTensor)\n",
    "        # pdb.set_trace()\n",
    "        current_labels = torch.from_numpy(self.labels[idx, pt_idxs].copy()\n",
    "                                         ).type(torch.LongTensor)\n",
    "\n",
    "        return current_points, current_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.points.shape[0] * self.data_precent)\n",
    "\n",
    "    def set_num_points(self, pts):\n",
    "        self.num_points = pts\n",
    "\n",
    "    def randomize(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "# 测试数据读取\n",
    "dset = Kitti3DSemSeg(16, train = True)\n",
    "print(dset[0])\n",
    "print(len(dset))\n",
    "dloader = torch.utils.data.DataLoader(dset, batch_size=32, shuffle=True)\n",
    "for i, data in enumerate(dloader, 0):\n",
    "    inputs, labels = data\n",
    "    if i == len(dloader) - 1:\n",
    "        print(inputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，原本的```.label```中存储的label并不能直接用于训练，我们需要通过```semantic-kitti.yaml```文件中的learning_map来得到映射，将其转换为能够直接用于训练的label。反之，由于semantic-kitti-api的可视化工具中需要使用原始的label，因此，在后续的测试中，我们需要将训练得到的label通过learning_map_inv来映射为原始的label。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 训练\n",
    "在构建好数据读取、定义好模型之后，可以开始训练了。为了方便实验，PointNet2_PyTorch原作者构建和使用了多个pytorch的工具，用来便捷地开始训练和测试。这里我们也继承了他的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_sched\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "# 添加模型，上面介绍的模型结构在文件夹models下有对应的.py文件\n",
    "from pointnet2.models import Pointnet2SemMSG as Pointnet\n",
    "from pointnet2.models.pointnet2_msg_sem import model_fn_decorator\n",
    "from pointnet2.data.KittiLoader import Kitti3DSemSeg\n",
    "import pointnet2.utils.pytorch_utils as pt_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch大小\n",
    "batch_size = 8\n",
    "# 每帧点云中有多少点\n",
    "num_points = 100592\n",
    "# L2正则化系数\n",
    "weight_decay = 0\n",
    "# 学习率\n",
    "lr = 1e-3\n",
    "# 学习率衰减指数\n",
    "lr_decay = 0.5\n",
    "# 学习率衰减step\n",
    "decay_step = 2e3\n",
    "# batch norm monentum\n",
    "bn_momentum = 0.9\n",
    "# batch norm momentum decay gamma\n",
    "bn_decay = 0.5\n",
    "# epoch\n",
    "epoches = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "(10059200, 3)\n",
      "9799219\n",
      "(100, 100592, 9)\n",
      "(100, 100592)\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# 测试集\n",
    "test_set = Kitti3DSemSeg(num_points, train=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size,shuffle=True,pin_memory=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "(10059200, 3)\n",
      "9799219\n",
      "(100, 100592, 9)\n",
      "(100, 100592)\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "train_set = Kitti3DSemSeg(num_points)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模型\n",
    "model = Pointnet(num_classes=34, input_channels=6, use_xyz=True)\n",
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clip = 1e-5\n",
    "bnm_clip = 1e-2\n",
    "\n",
    "# 设置optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=lr, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# 设置lr_lbmd和bnm_lmdd\n",
    "lr_lbmd = lambda it: max(lr_decay**(int(it * batch_size / decay_step)), lr_clip / lr)\n",
    "bnm_lmbd = lambda it: max(bn_momentum * bn_decay**(int(it * batch_size / decay_step)), bnm_clip)\n",
    "\n",
    "# 设置训练计划\n",
    "lr_scheduler = lr_sched.LambdaLR(optimizer, lr_lbmd)\n",
    "bnm_scheduler = pt_utils.BNMomentumScheduler(model, bnm_lmbd)\n",
    "start_epoch = 1\n",
    "best_prec = 0\n",
    "best_loss = 1e10\n",
    "\n",
    "# 设置全连接层\n",
    "model_fn = model_fn_decorator(nn.CrossEntropyLoss())\n",
    "\n",
    "# 设置viz\n",
    "# viz = pt_utils.VisdomViz(port=8097)\n",
    "# viz.text(str('PointNet2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   0%|          | 0/600 [00:00<?, ?it/s]\n",
      "train:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "train:   8%|▊         | 1/13 [00:01<00:13,  1.13s/it]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:01<?, ?it/s]3s/it, total_it=1]\u001b[A\n",
      "train:  15%|█▌        | 2/13 [00:02<00:11,  1.06s/it, total_it=1]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:02<?, ?it/s]6s/it, total_it=2]\u001b[A\n",
      "train:  23%|██▎       | 3/13 [00:02<00:09,  1.01it/s, total_it=2]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:02<?, ?it/s]1it/s, total_it=3]\u001b[A\n",
      "train:  31%|███       | 4/13 [00:03<00:08,  1.09it/s, total_it=3]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:03<?, ?it/s]9it/s, total_it=4]\u001b[A\n",
      "train:  38%|███▊      | 5/13 [00:04<00:06,  1.15it/s, total_it=4]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:04<?, ?it/s]5it/s, total_it=5]\u001b[A\n",
      "train:  46%|████▌     | 6/13 [00:05<00:05,  1.19it/s, total_it=5]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:05<?, ?it/s]9it/s, total_it=6]\u001b[A\n",
      "train:  54%|█████▍    | 7/13 [00:05<00:04,  1.23it/s, total_it=6]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:05<?, ?it/s]3it/s, total_it=7]\u001b[A\n",
      "train:  62%|██████▏   | 8/13 [00:06<00:04,  1.25it/s, total_it=7]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:06<?, ?it/s]5it/s, total_it=8]\u001b[A\n",
      "train:  69%|██████▉   | 9/13 [00:07<00:03,  1.27it/s, total_it=8]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:07<?, ?it/s]7it/s, total_it=9]\u001b[A\n",
      "train:  77%|███████▋  | 10/13 [00:08<00:02,  1.28it/s, total_it=9]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:08<?, ?it/s]28it/s, total_it=10]\u001b[A\n",
      "train:  85%|████████▍ | 11/13 [00:08<00:01,  1.29it/s, total_it=10]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:08<?, ?it/s]29it/s, total_it=11]\u001b[A\n",
      "train:  92%|█████████▏| 12/13 [00:09<00:00,  1.29it/s, total_it=11]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:09<?, ?it/s]29it/s, total_it=12]\u001b[A\n",
      "train: 100%|██████████| 13/13 [00:10<00:00,  1.39it/s, total_it=12]\u001b[A\n",
      "epochs:   0%|          | 0/600 [00:10<?, ?it/s]39it/s, total_it=13]\u001b[A\n",
      "                                                                   \u001b[A\n",
      "val:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "val:   8%|▊         | 1/13 [00:00<00:05,  2.05it/s]\u001b[A\n",
      "val:  15%|█▌        | 2/13 [00:01<00:05,  1.88it/s]\u001b[A\n",
      "val:  23%|██▎       | 3/13 [00:01<00:05,  1.74it/s]\u001b[A\n",
      "val:  31%|███       | 4/13 [00:02<00:04,  1.81it/s]\u001b[A\n",
      "val:  38%|███▊      | 5/13 [00:02<00:04,  1.87it/s]\u001b[A\n",
      "val:  46%|████▌     | 6/13 [00:03<00:03,  1.91it/s]\u001b[A\n",
      "val:  54%|█████▍    | 7/13 [00:03<00:03,  1.94it/s]\u001b[A\n",
      "val:  62%|██████▏   | 8/13 [00:04<00:02,  1.92it/s]\u001b[A\n",
      "val:  69%|██████▉   | 9/13 [00:04<00:02,  1.90it/s]\u001b[A\n",
      "val:  77%|███████▋  | 10/13 [00:05<00:01,  1.94it/s]\u001b[A\n",
      "val:  85%|████████▍ | 11/13 [00:05<00:01,  1.96it/s]\u001b[A\n",
      "val:  92%|█████████▏| 12/13 [00:06<00:00,  1.98it/s]\u001b[A\n",
      "val: 100%|██████████| 13/13 [00:06<00:00,  2.09it/s]\u001b[A\n",
      "                                                    \u001b[A\n",
      "train:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epochs:   0%|          | 1/600 [00:17<2:55:10, 17.55s/it]]\u001b[A\n",
      "train:   8%|▊         | 1/13 [00:00<00:09,  1.30it/s, total_it=13]\u001b[A\n",
      "epochs:   0%|          | 1/600 [00:18<2:55:10, 17.55s/it]al_it=14]\u001b[A\n",
      "train:  15%|█▌        | 2/13 [00:01<00:08,  1.31it/s, total_it=14]\u001b[A\n",
      "epochs:   0%|          | 1/600 [00:19<3:15:27, 19.58s/it]al_it=15]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4a89ec8f2809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kitti_semantic/Pointnet2_PyTorch-master/pointnet2/utils/pytorch_utils/pytorch_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_it, start_epoch, n_epochs, train_loader, test_loader, best_loss)\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m                     \u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kitti_semantic/Pointnet2_PyTorch-master/pointnet2/utils/pytorch_utils/pytorch_utils.py\u001b[0m in \u001b[0;36m_train_it\u001b[0;34m(self, it, batch)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kitti_semantic/Pointnet2_PyTorch-master/pointnet2/models/pointnet2_msg_sem.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(model, data, epoch, eval)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         return ModelReturn(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer = pt_utils.Trainer(\n",
    "    model,\n",
    "    model_fn,\n",
    "    optimizer,\n",
    "    checkpoint_name=\"/kitti_semantic/Pointnet2_PyTorch-master/pointnet2/train/checkpoints/pointnet2_smeseg\",\n",
    "    best_name=\"/kitti_semantic/Pointnet2_PyTorch-master/pointnet2/train/checkpoints/pointnet2_semseg_best\",\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    bnm_scheduler=bnm_scheduler\n",
    "    # viz=viz\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    0,\n",
    "    start_epoch,\n",
    "    epoches,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    best_loss=best_loss\n",
    ")\n",
    "\n",
    "if start_epoch == epoches:\n",
    "    _ = trainer.eval_epoch(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 测试\n",
    "在完成训练之后，我们可以使用数据进行测试了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置预测结果输出文件\n",
    "preds_path = '/kitti_semantic/dataset/sequences/00/preds1'\n",
    "# batch_size\n",
    "batch_size = 1\n",
    "# num_points\n",
    "num_points = 100592\n",
    "# device_name\n",
    "device_name = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 读取测试集数据\n",
    "为了减轻服务器负担，本实验选择训练集当做测试集，读者也可将Kitti中的别的数据作为测试集进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "(10059200, 3)\n",
      "9799219\n",
      "(100, 100592, 9)\n",
      "(100, 100592)\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "test_set = Kitti3DSemSeg(num_points, train = False)\n",
    "test_loader = DataLoader(test_set, batch_size = batch_size, \n",
    "                         shuffle = False, \n",
    "                         pin_memory = True,\n",
    "                         num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 读取模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device_name)\n",
    "model = Pointnet(num_classes = 34, input_channels = 6, use_xyz = True)\n",
    "model.load_state_dict(torch.load(\n",
    "    '/kitti_semantic/Pointnet2_PyTorch-master/pointnet2/train/pointnet2_semseg_best.pth.tar', \n",
    "    map_location = device\n",
    ")['model_state'])\n",
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 进行测试\n",
    "进行测试，并将测试结果以Semantic-Kitti的.label文件格式存储下来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取learning_map_inv，用于将测试得到的label转换为可视化工具需要的label\n",
    "config_path = '/kitti_semantic/Pointnet2_PyTorch-master/pointnet2/data/kitti_api/config/semantic-kitti.yaml'\n",
    "CFG = yaml.safe_load(open(config_path, 'r'))\n",
    "class_inv_map = CFG['learning_map_inv']\n",
    "# ground truth文件夹\n",
    "label_path = '/kitti_semantic/dataset/sequences/00/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0 Segmantion Accuracy: 0.6839    \n",
      "Frame 1 Segmantion Accuracy: 0.6570    \n",
      "Frame 2 Segmantion Accuracy: 0.7398    \n",
      "Frame 3 Segmantion Accuracy: 0.7500    \n",
      "Frame 4 Segmantion Accuracy: 0.8149    \n",
      "Frame 5 Segmantion Accuracy: 0.8684    \n",
      "Frame 6 Segmantion Accuracy: 0.8777    \n",
      "Frame 7 Segmantion Accuracy: 0.8575    \n",
      "Frame 8 Segmantion Accuracy: 0.8554    \n",
      "Frame 9 Segmantion Accuracy: 0.8136    \n",
      "Frame 10 Segmantion Accuracy: 0.8787    \n",
      "Frame 11 Segmantion Accuracy: 0.8565    \n",
      "Frame 12 Segmantion Accuracy: 0.8901    \n",
      "Frame 13 Segmantion Accuracy: 0.9167    \n",
      "Frame 14 Segmantion Accuracy: 0.8689    \n",
      "Frame 15 Segmantion Accuracy: 0.9190    \n",
      "Frame 16 Segmantion Accuracy: 0.9015    \n",
      "Frame 17 Segmantion Accuracy: 0.9237    \n",
      "Frame 18 Segmantion Accuracy: 0.8910    \n",
      "Frame 19 Segmantion Accuracy: 0.8250    \n",
      "Frame 20 Segmantion Accuracy: 0.8098    \n",
      "Frame 21 Segmantion Accuracy: 0.7853    \n",
      "Frame 22 Segmantion Accuracy: 0.7385    \n",
      "Frame 23 Segmantion Accuracy: 0.8200    \n",
      "Frame 24 Segmantion Accuracy: 0.8638    \n",
      "Frame 25 Segmantion Accuracy: 0.9046    \n",
      "Frame 26 Segmantion Accuracy: 0.9133    \n",
      "Frame 27 Segmantion Accuracy: 0.8743    \n",
      "Frame 28 Segmantion Accuracy: 0.8901    \n",
      "Frame 29 Segmantion Accuracy: 0.9232    \n",
      "Frame 30 Segmantion Accuracy: 0.9101    \n",
      "Frame 31 Segmantion Accuracy: 0.8986    \n",
      "Frame 32 Segmantion Accuracy: 0.9214    \n",
      "Frame 33 Segmantion Accuracy: 0.9319    \n",
      "Frame 34 Segmantion Accuracy: 0.9246    \n",
      "Frame 35 Segmantion Accuracy: 0.9285    \n",
      "Frame 36 Segmantion Accuracy: 0.9232    \n",
      "Frame 37 Segmantion Accuracy: 0.9284    \n",
      "Frame 38 Segmantion Accuracy: 0.9361    \n",
      "Frame 39 Segmantion Accuracy: 0.9388    \n",
      "Frame 40 Segmantion Accuracy: 0.9453    \n",
      "Frame 41 Segmantion Accuracy: 0.9378    \n",
      "Frame 42 Segmantion Accuracy: 0.9415    \n",
      "Frame 43 Segmantion Accuracy: 0.9379    \n",
      "Frame 44 Segmantion Accuracy: 0.9234    \n",
      "Frame 45 Segmantion Accuracy: 0.8963    \n",
      "Frame 46 Segmantion Accuracy: 0.9050    \n",
      "Frame 47 Segmantion Accuracy: 0.9041    \n",
      "Frame 48 Segmantion Accuracy: 0.9374    \n",
      "Frame 49 Segmantion Accuracy: 0.9303    \n",
      "Frame 50 Segmantion Accuracy: 0.9347    \n",
      "Frame 51 Segmantion Accuracy: 0.9386    \n",
      "Frame 52 Segmantion Accuracy: 0.9161    \n",
      "Frame 53 Segmantion Accuracy: 0.9228    \n",
      "Frame 54 Segmantion Accuracy: 0.9052    \n",
      "Frame 55 Segmantion Accuracy: 0.9068    \n",
      "Frame 56 Segmantion Accuracy: 0.9050    \n",
      "Frame 57 Segmantion Accuracy: 0.8736    \n",
      "Frame 58 Segmantion Accuracy: 0.8839    \n",
      "Frame 59 Segmantion Accuracy: 0.8538    \n",
      "Frame 60 Segmantion Accuracy: 0.8216    \n",
      "Frame 61 Segmantion Accuracy: 0.8731    \n",
      "Frame 62 Segmantion Accuracy: 0.8949    \n",
      "Frame 63 Segmantion Accuracy: 0.9184    \n",
      "Frame 64 Segmantion Accuracy: 0.9331    \n",
      "Frame 65 Segmantion Accuracy: 0.9332    \n",
      "Frame 66 Segmantion Accuracy: 0.8990    \n",
      "Frame 67 Segmantion Accuracy: 0.8964    \n",
      "Frame 68 Segmantion Accuracy: 0.9180    \n",
      "Frame 69 Segmantion Accuracy: 0.8899    \n",
      "Frame 70 Segmantion Accuracy: 0.9045    \n",
      "Frame 71 Segmantion Accuracy: 0.8536    \n",
      "Frame 72 Segmantion Accuracy: 0.8738    \n",
      "Frame 73 Segmantion Accuracy: 0.9038    \n",
      "Frame 74 Segmantion Accuracy: 0.8708    \n",
      "Frame 75 Segmantion Accuracy: 0.8949    \n",
      "Frame 76 Segmantion Accuracy: 0.8849    \n",
      "Frame 77 Segmantion Accuracy: 0.8342    \n",
      "Frame 78 Segmantion Accuracy: 0.9078    \n",
      "Frame 79 Segmantion Accuracy: 0.8951    \n",
      "Frame 80 Segmantion Accuracy: 0.8793    \n",
      "Frame 81 Segmantion Accuracy: 0.8826    \n",
      "Frame 82 Segmantion Accuracy: 0.8532    \n",
      "Frame 83 Segmantion Accuracy: 0.9130    \n",
      "Frame 84 Segmantion Accuracy: 0.8476    \n",
      "Frame 85 Segmantion Accuracy: 0.8382    \n",
      "Frame 86 Segmantion Accuracy: 0.8519    \n",
      "Frame 87 Segmantion Accuracy: 0.8593    \n",
      "Frame 88 Segmantion Accuracy: 0.8393    \n",
      "Frame 89 Segmantion Accuracy: 0.7957    \n",
      "Frame 90 Segmantion Accuracy: 0.8496    \n",
      "Frame 91 Segmantion Accuracy: 0.8713    \n",
      "Frame 92 Segmantion Accuracy: 0.8530    \n",
      "Frame 93 Segmantion Accuracy: 0.8356    \n",
      "Frame 94 Segmantion Accuracy: 0.8556    \n",
      "Frame 95 Segmantion Accuracy: 0.8450    \n",
      "Frame 96 Segmantion Accuracy: 0.7647    \n",
      "Frame 97 Segmantion Accuracy: 0.7915    \n",
      "Frame 98 Segmantion Accuracy: 0.7820    \n",
      "Frame 99 Segmantion Accuracy: 0.7496    \n"
     ]
    }
   ],
   "source": [
    "# 对测试数据进行训练\n",
    "for i,data in enumerate(test_loader):\n",
    "    f = os.path.join(preds_path, str(i).zfill(6) + '.label')\n",
    "    f_l = os.path.join(label_path, str(i).zfill(6) + '.label')\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device, non_blocking = True)\n",
    "    labels = labels.to(device, non_blocking = True)\n",
    "    \n",
    "    preds = model(inputs)\n",
    "    _, classes = torch.max(preds, -1)\n",
    "\n",
    "    acc = (classes == labels).float().sum() / labels.numel()\n",
    "    print('Frame %d Segmantion Accuracy: %-10.6s' %  (i, acc.item()))\n",
    "\n",
    "    classes = classes.clone().cpu().numpy()\n",
    "    classes = classes.reshape(num_points).tolist()\n",
    "    classes_bin = []\n",
    "    for i in range(len(classes)):\n",
    "        classes_bin.append(class_inv_map[classes[i]])\n",
    "\n",
    "    # 找出与源点云的数量的差值，补齐没有被选作为测试的点，可视化用\n",
    "    k = len(np.fromfile(f_l,dtype = np.uint32).tolist()) - len(classes)\n",
    "    for i in range(k):\n",
    "        classes_bin.append(0)\n",
    "    \n",
    "    # 将测试结果写入.label文件中\n",
    "    classes_bin = np.array(classes_bin, dtype = np.uint32)\n",
    "   #  print(classes_bin.shape)\n",
    "    classes_bin.tofile(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 可视化结果展示： \n",
    "\n",
    "我们随机选择了三帧点云来进行可视化，其结果如下：\n",
    "\n",
    "frame_0:\n",
    "![frame_0](images/frame_0_pred.png)\n",
    "\n",
    "frame_51:\n",
    "![frame_0](images/frame_51_pred.png)\n",
    "\n",
    "frame_99:\n",
    "![frame_0](images/frame_99_pred.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PointNet2.0",
   "language": "python",
   "name": "pointnet2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
